{
 "id": "716644",
 "text": "In statistics, the method of moments is a method of estimation of population parameters. ==Method== Suppose that the problem is to estimate k unknown parameters \\theta_{1}, \\theta_2, \\dots, \\theta_k describing the distribution f_W(w; \\theta) of the random variable W.K. O. Bowman and L. R. Shenton, \"Estimator: Method of Moments\", pp 2092â€“2098, Encyclopedia of statistical sciences, Wiley (1998). Suppose the first k moments of the true distribution (the \"population moments\") can be expressed as functions of the \\thetas: : \\begin{align} \\mu_1 & \\equiv \\operatorname E[W]=g_1(\\theta_1, \\theta_2, \\ldots, \\theta_k) , \\\\\\\\[4pt] \\mu_2 & \\equiv \\operatorname E[W^2]=g_2(\\theta_1, \\theta_2, \\ldots, \\theta_k), \\\\\\ & \\,\\,\\, \\vdots \\\\\\ \\mu_k & \\equiv \\operatorname E[W^k]=g_k(\\theta_1, \\theta_2, \\ldots, \\theta_k). \\end{align} Suppose a sample of size n is drawn, and it leads to the values w_1, \\dots, w_n. For j=1,\\dots,k, let :\\widehat\\mu_j = \\frac{1}{n} \\sum_{i=1}^n w_i^j be the j-th sample moment, an estimate of \\mu_j. The method of moments estimator for \\theta_1, \\theta_2, \\ldots, \\theta_k denoted by \\widehat\\theta_1, \\widehat\\theta_2, \\dots, \\widehat\\theta_k is defined as the solution (if there is one) to the equations: : \\begin{align} \\widehat \\mu_1 & = g_1(\\widehat\\theta_1, \\widehat\\theta_2, \\ldots, \\widehat\\theta_k), \\\\\\\\[4pt] \\widehat \\mu_2 & = g_2(\\widehat\\theta_1, \\widehat\\theta_2, \\ldots, \\widehat\\theta_k), \\\\\\ & \\,\\,\\, \\vdots \\\\\\ \\widehat \\mu_k & = g_k(\\widehat\\theta_1, \\widehat\\theta_2, \\ldots, \\widehat\\theta_k). \\end{align} ==Reasons to use it== The method of moments is simple and gets consistent estimators (under very weak assumptions). However, these estimators are often biased. ==References== Category:Statistics",
 "title": "Method of moments (statistics)"
}
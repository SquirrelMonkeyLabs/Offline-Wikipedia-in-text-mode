{
 "id": "470230",
 "text": "Statistics uses variables to describe a measurement. Such a variable is called statistically significant if under a certain status quo assumption, the probability of obtaining its outcome (or a more extreme outcome) is less than a given value. Statistical significance is hence a way of determining the unlikeliness of an experimental result—when a certain status quo assumption is assumed to be true. Statistical hypothesis tests are used to check significance. The concept of statistical significance was originated by Ronald Fisher in his 1925 publication, Statistical Methods for Research Workers, when he developed statistical hypothesis testing (which he described as \"tests of significance\"). Fisher suggested a probability of one in twenty (0.05 or 5%)—as a convenient cutoff level to reject the null hypothesis. In their 1933 paper, Jerzy Neyman and Egon Pearson recommended that the significance level (for example 0.05), which they called α, be set before any data collection. Despite his initial suggestion of 0.05 as a significance level, Fisher did not intend this cutoff value to be fixed. In his 1956 publication Statistical methods and scientific inference, he recommended that significant levels be set according to specific circumstances. == Related pages == * p-value ==References== Category:Statistics",
 "title": "Statistical significance"
}